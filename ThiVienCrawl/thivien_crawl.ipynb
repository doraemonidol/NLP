{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3rKMFPsqDWep"
      },
      "outputs": [],
      "source": [
        "# DATA_PATH = r'/content/gdrive/MyDrive/Colab/thivien-crawl/'\n",
        "DATA_PATH = r'./'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "evl56S_-DjE5"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# json\n",
        "# import class to store the content in json format\n",
        "import json\n",
        "from time import sleep\n",
        "from random import randint\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Data:\n",
        "    def __init__(self):\n",
        "        self.data = {\n",
        "            \"title_cn\": \"\",\n",
        "            \"title_hv\": \"\",\n",
        "            \"title_vi\": \"\",\n",
        "            \"content_cn\": \"\",\n",
        "            \"content_hv\": \"\",\n",
        "            \"content_vi\": \"\",\n",
        "            \"description\": \"\",\n",
        "            \"member_translation\": [],\n",
        "            \"url\": \"\",\n",
        "            \"tags\": []\n",
        "        }\n",
        "        self.mask = {\n",
        "            \"title_cn\": False,\n",
        "            \"title_hv\": False,\n",
        "            \"title_vi\": False,\n",
        "            \"content_cn\": False,\n",
        "            \"content_hv\": False,\n",
        "            \"content_vi\": False,\n",
        "            \"description\": False,\n",
        "            \"member_translation\": 0,\n",
        "            \"url\": False,\n",
        "            \"tags\": False\n",
        "        }\n",
        "\n",
        "    def set(self, key, value):\n",
        "        if key == \"member_translation\":\n",
        "            self.data[key].append(value)\n",
        "            self.mask[key] += 1\n",
        "        elif key == \"tags\":\n",
        "            self.data[key].append(value)\n",
        "            self.mask[key] = True\n",
        "        else:\n",
        "            self.data[key] = value\n",
        "            self.mask[key] = True\n",
        "\n",
        "    def get(self, key):\n",
        "        return self.data[key]\n",
        "\n",
        "class Crawler:\n",
        "    def __init__(self):\n",
        "        self.parsed_content = []\n",
        "\n",
        "    def __fetch__(self):\n",
        "        r = requests.get(self.url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        r.encoding = 'utf-8'\n",
        "        html = r.text\n",
        "        return html\n",
        "\n",
        "    def to(self, url):\n",
        "        self.url = url\n",
        "        self.html = self.__fetch__()\n",
        "        return self\n",
        "\n",
        "    def get_random(self):\n",
        "        pass\n",
        "\n",
        "    def parse(self):\n",
        "        soup = BeautifulSoup(self.html, 'html.parser')\n",
        "        for br in soup.find_all(\"br\"):\n",
        "             br.insert_after(\"\\n\")\n",
        "        data = Data()\n",
        "        data.set(\"tags\", soup.find_all(\"meta\", {\"name\": \"keywords\"})[0][\"content\"].split(\",\"))\n",
        "        data.set(\"url\", self.url)\n",
        "        full_title = soup.find_all(\"h1\")[0].text\n",
        "        try:\n",
        "            extracted_title_vi = full_title.split(\" • \")[1]\n",
        "            data.set(\"title_vi\", extracted_title_vi)\n",
        "        except:\n",
        "            pass\n",
        "        extracted_title_hv = full_title.split(\" • \")[0].split(\" \")[-1]\n",
        "        extracted_title_cn = ' '.join(full_title.split(\" • \")[0].split(\" \")[0:-1])\n",
        "        data.set(\"title_cn\", extracted_title_cn)\n",
        "        data.set(\"title_hv\", extracted_title_hv)\n",
        "        # <p class=\"HanChinese transcriptable\" data-han-lang=\"hv\" lang=\"zh-Hant\">刳血書成欲寄音，<br/>孤飛寒雁塞雲深。<br/>幾家愁對今霄月，<br/>兩處茫然一種心。</p>\n",
        "\n",
        "        try:\n",
        "            content_cn = soup.find_all(\"p\", {\"lang\": \"zh-Hant\"})[0].text\n",
        "            data.set(\"content_cn\", content_cn)\n",
        "        except:\n",
        "            raise Exception(\"Not Han Chinese content found\")\n",
        "\n",
        "        # <p class=\"HanChinese transcriptable\" data-han-lang=\"hv\" lang=\"zh-Hant\">刳血書成欲寄音，<br/>孤飛寒雁塞雲深。<br/>幾家愁對今霄月，<br/>兩處茫然一種心。</p><p>&nbsp;</p><h4><strong>Ai phu lỗ</strong></h4><p>Khô huyết thư thành dục ký âm,<br/>Cô phi hàn nhạn tái vân thâm.<br/>Kỷ gia sầu đối kim tiêu nguyệt,<br/>Lưỡng xứ mang nhiên nhất chủng tâm.</p><p>&nbsp;</p>\n",
        "        #                                 <h4><strong>Dịch nghĩa</strong></h4>\n",
        "        #                                 <p>Chích máu viết thư muốn gửi lời,<br/>Cánh nhạn lạnh lùng bay xuyên vào đám mây ngoài quan ải.<br/>Bao nhiêu nhà buồn ngắm bóng trăng đêm nay?<br/>Đôi nơi xa cách nhưng tấm lòng nhớ thương vẫn chỉ là một.</p>\n",
        "        # content_cn = 刳血書成欲寄音，<br/>孤飛寒雁塞雲深。<br/>幾家愁對今霄月，<br/>兩處茫然一種心。\n",
        "        # content_hv = Khô huyết thư thành dục ký âm,<br/>Cô phi hàn nhạn tái vân thâm.<br/>Kỷ gia sầu đối kim tiêu nguyệt,<br/>Lưỡng xứ mang nhiên nhất chủng tâm.\n",
        "        # content_vi = Chích máu viết thư muốn gửi lời,<br/>Cánh nhạn lạnh lùng bay xuyên vào đám mây ngoài quan ải.<br/>Bao nhiêu nhà buồn ngắm bóng trăng đêm nay?<br/>Đôi nơi xa cách nhưng tấm lòng nhớ thương vẫn chỉ là một.\n",
        "        # The content_vi might be optional, based on <h4><strong>Dịch nghĩa</strong></h4> appearance\n",
        "\n",
        "        content_hv = soup.find_all(\"p\", {\"lang\": \"zh-Hant\"})[0].find_next_sibling(\"p\").find_next_sibling(\"p\").text\n",
        "        data.set(\"content_hv\", content_hv)\n",
        "\n",
        "        try:\n",
        "            # From soup.find_all(\"p\", {\"lang\": \"zh-Hant\"})[0]\n",
        "            # Find <h4><strong>Dịch nghĩa</strong></h4>\n",
        "            # Then find the next <p> tag\n",
        "            # Then get the text\n",
        "            content_vi = soup.find_all(\"p\", {\"lang\": \"zh-Hant\"})[0].find_next_sibling(\"h4\").find_next_sibling(\"h4\").find_next_sibling(\"p\").text\n",
        "            data.set(\"content_vi\", content_vi)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            # From soup.find_all(\"p\", {\"lang\": \"zh-Hant\"})[0]\n",
        "            # Find <h4><strong>Dịch nghĩa</strong></h4>\n",
        "            # Then find the next <p> tag\n",
        "            # Then get the text\n",
        "            description = soup.find_all(\"div\", {\"class\": \"poem-content\"})[0].find_next_sibling(\"div\").text\n",
        "            print(description)\n",
        "            data.set(\"description\", description)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # There might be multiple member translations\n",
        "        # Each of them has:\n",
        "        # <h4 class=\"post-title\"><a id=\"REPLY78708\"></a>Bản dịch của <a href=\"/translator/Ho%C3%A0ng+%C4%90%C3%ACnh+Thi\">Hoàng Đình Thi</a></h4><p class=\"post-info small\">Gửi bởi <a href=\"/Ho%C3%A0ng-%C4%90%C3%ACnh-Thi/member-GI9qZKC9bqDPcxa9lttDlA\" >Hoàng Đình Thi</a> ngày 12/01/2022 16:32<br/>Đã sửa 2 lần, lần cuối bởi <a href=\"/Ho%C3%A0ng-%C4%90%C3%ACnh-Thi/member-GI9qZKC9bqDPcxa9lttDlA\" >Hoàng Đình Thi</a> ngày 29/11/2022 05:06</p></div>\n",
        "        #                 </div><div class=\"post-content\"><p>Chích máu đề thư muốn gửi lời,<br/>Nhạn bay lẻ bóng chốn biên khơi.<br/>Bao nhà sầu não nhìn trăng lạnh,<br/>Một giọt lệ rơi, hai phuơng trời.</p>\n",
        "\n",
        "        all_member_translations_place = soup.find_all(\"div\", {\"class\": \"post-content\"})\n",
        "        for member_translation_place in all_member_translations_place:\n",
        "            member_translation = member_translation_place.find_all(\"p\")[0].text\n",
        "            if member_translation.count(\"\\n\") != content_cn.count(\"\\n\"):\n",
        "                continue\n",
        "            data.set(\"member_translation\", member_translation)\n",
        "\n",
        "        self.parsed_content.append(data)\n",
        "        return self\n",
        "\n",
        "    def save(self, file_name):\n",
        "        # Save the parsed content to a file with json format with utf-8 encoding\n",
        "        that_ngon_tu_tuyet = []\n",
        "        ngu_ngon_tu_tuyet = []\n",
        "        that_ngon_bat_cu = []\n",
        "        ngu_ngon_bat_cu = []\n",
        "        others = []\n",
        "        for data in self.parsed_content:\n",
        "            if \"thất ngôn tứ tuyệt\" in [tag.lower() for tag in data.get(\"tags\")[0]]:\n",
        "                that_ngon_tu_tuyet.append(data.data)\n",
        "            elif \"ngũ ngôn tứ tuyệt\" in [tag.lower() for tag in data.get(\"tags\")[0]]:\n",
        "                ngu_ngon_tu_tuyet.append(data.data)\n",
        "            elif \"thất ngôn bát cú\" in [tag.lower() for tag in data.get(\"tags\")[0]]:\n",
        "                that_ngon_bat_cu.append(data.data)\n",
        "            elif \"ngũ ngôn bát cú\" in [tag.lower() for tag in data.get(\"tags\")[0]]:\n",
        "                ngu_ngon_bat_cu.append(data.data)\n",
        "            else:\n",
        "                others.append(data.data)\n",
        "        with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump({\n",
        "                \"thất ngôn tứ tuyệt\": that_ngon_tu_tuyet,\n",
        "                \"ngũ ngôn tứ tuyệt\": ngu_ngon_tu_tuyet,\n",
        "                \"thất ngôn bát cú\": that_ngon_bat_cu,\n",
        "                \"ngũ ngôn bát cú\": ngu_ngon_bat_cu,\n",
        "                \"others\": others\n",
        "            }, f, ensure_ascii=False)\n",
        "        return self\n",
        "\n",
        "# link = \"https://www.thivien.net/V%C6%B0%C6%A1ng-An-Th%E1%BA%A1ch/Ho%C3%A0i-Chung-s%C6%A1n/poem-EJVsHAfe5zRvjli0I3auRw\"\n",
        "# c = Crawler()\n",
        "# c.to(link).parse()\n",
        "# c.save(\"/Users/hoangtheanh/Documents/thivien-crawler/data/test.json\")\n",
        "\n",
        "def automatic_crawl(links, file_name):\n",
        "    c = Crawler()\n",
        "    i = 1\n",
        "    try:\n",
        "        for _ in range(1):\n",
        "            while i <= len(links):\n",
        "                if (links[i-1] in ['T%E1%BB%B1-tr%C3%A0o-b%C3%A0i-2/poem-aAhxiiaa9U2WpLwFUSATfg']):\n",
        "                    i += 1\n",
        "                    continue\n",
        "                try:\n",
        "                    link = \"https://www.thivien.net\" + links[i - 1]\n",
        "                    # link = r\"https://www.thivien.net/%C4%90%E1%BB%97-Ph%E1%BB%A7/Dinh-%E1%BB%91c/poem-WkmQ5ViHVHWdu6nUI85PMA\"\n",
        "                    print(link)\n",
        "                    c.to(link).parse()\n",
        "                    # sleep(randint(1, 3))\n",
        "                    i += 1\n",
        "                except Exception as e:\n",
        "                    if \"Not Han Chinese content found\" in str(e):\n",
        "                        i += 1\n",
        "                        continue\n",
        "                    else:\n",
        "                        print(e)\n",
        "                        sleep(240)\n",
        "                        break\n",
        "        c.save(file_name)\n",
        "    except:\n",
        "        c.save(file_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-l2W4-mM-mr"
      },
      "outputs": [],
      "source": [
        "with open(f\"{DATA_PATH}0.txt\", \"r\", encoding = 'utf-8') as f:\n",
        "    list_of_poems = f.readlines()\n",
        "    links = [poem.split(\"\\t\")[-1].strip() for poem in list_of_poems]\n",
        "\n",
        "automatic_crawl(links, f\"{DATA_PATH}thivien.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ggCWIt9GXLp",
        "outputId": "fa07770a-4c69-43f7-dc02-1ac71e61e155"
      },
      "outputs": [],
      "source": [
        "link = \"https://www.thivien.net/V%C6%B0%C6%A1ng-An-Th%E1%BA%A1ch/Ho%C3%A0i-Chung-s%C6%A1n/poem-EJVsHAfe5zRvjli0I3auRw\"\n",
        "c = Crawler()\n",
        "c.to(link).parse()\n",
        "c.save(f\"{DATA_PATH}test.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split poem.txt.2 into 4 files\n",
        "with open('poem.txt.2', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    n = len(lines) // 4\n",
        "    for i in range(4):\n",
        "        with open(f'{i}.txt', 'w') as f:\n",
        "            f.writelines(lines[i*n:(i+1)*n])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
